# -*- coding: utf-8 -*-
"""EMPLOYEEFINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10MxGbCXaCln7G1XM04zA_Py8g5XQjX_V
"""



import pandas as pd

data=pd.read_csv('/content/adult 3.csv')

data

data.shape

data.head()

data.tail()

data.isna()

data.isna().sum()

print(data.occupation.value_counts())

print(data.gender.value_counts())

print(data.education.value_counts())

print(data.workclass.value_counts())

print(data['marital-status'].value_counts())

data.occupation.replace({'?':'others'},inplace=True)

print(data.occupation.value_counts())

data.workclass.replace({'?':'Notlisted'},inplace=True)

print(data.workclass.value_counts())

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']

print(data['workclass'].value_counts())

data.shape

data=data[data['education']!='5th-6th']
data=data[data['education']!='1st-4th']
data=data[data['education']!='Preschool']

data.shape

data.drop(columns=['education'],inplace=True)

data

import matplotlib.pyplot as plt
plt.boxplot(  data['age'])
plt.show()

data = data[(data['age'] <= 75) & (data['age'] >= 17)]

import matplotlib.pyplot as plt
plt.boxplot(  data['age'])
plt.show()

from sklearn.preprocessing  import LabelEncoder
encoder=LabelEncoder()
data['marital-status']=encoder.fit_transform(data['marital-status'])
data['occupation']=encoder.fit_transform(data['occupation'])
data['relationship']=encoder.fit_transform(data['relationship'])
data['race']=encoder.fit_transform(data['race'])
data['gender']=encoder.fit_transform(data['gender'])
data['native-country']=encoder.fit_transform(data['native-country'])
data['workclass']=encoder.fit_transform(data['workclass'])

data

x=data.drop(columns=['income'])
y=data['income']
x

y

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x = scaler.fit_transform(x)
x

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=23,stratify=y)

xtrain

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(xtrain,ytrain)
predict=knn.predict(xtest)
predict

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict)

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(xtrain,ytrain)

predict1=lr.predict(xtest)
predict1

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict1)

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier(solver='adam',hidden_layer_sizes=(5,2),random_state=2,max_iter=2000)
clf.fit(xtrain,ytrain)
predict2=clf.predict(xtest)
predict2

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict2)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Split data
Xtrain, Xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

# Define models in a dictionary
models = {
    "LogisticRegression": LogisticRegression(),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "GradientBoosting": GradientBoostingClassifier(),
    "SVC": SVC()
}

results = {}

# Train and evaluate each model using a pipeline
for name, model in models.items():
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("model", model)
    ])
    pipe.fit(Xtrain, ytrain)
    y_pred = pipe.predict(Xtest)
    acc = accuracy_score(ytest, y_pred)
    results[name] = acc
    print(f"\n{name} Accuracy: {acc:.4f}")
    print(classification_report(ytest, y_pred))

import matplotlib.pyplot as plt

plt.bar(results.keys(), results.values(), color='skyblue')

plt.ylabel('Accuracy Score')

plt.title('Model Comparison')

plt.xticks(rotation=45)

plt.grid(True)


plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Define models
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

results = {}

# Train and evaluate
for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc
    print(f"{name}: {acc:.4f}")

# Get best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"\nâœ… Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# Save the best model
joblib.dump(best_model, "best_model.pkl")
print("âœ… Saved best model as best_model.pkl")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# 
# # Load the trained model
# model = joblib.load("best_model.pkl")
# 
# # You will also need to load your scaler and encoders here
# # scaler = joblib.load("scaler.pkl")
# # label_encoders = joblib.load("label_encoders.pkl") # Assuming you saved your encoders
# 
# st.set_page_config(page_title="Employee Salary Classification", page_icon="ðŸ’¼", layout="centered")
# 
# st.title("ðŸ’¼ Employee Salary Classification App")
# st.markdown("Predict whether an employee earns >50K or â‰¤50K based on input features.")
# 
# # Sidebar inputs - these must match your training feature columns
# st.sidebar.header("Input Employee Details")
# 
# age = st.sidebar.slider("Age", 17, 75, 30) # Adjusted age range based on your filtering
# workclass = st.sidebar.selectbox("Workclass", [
#     "Private", "Self-emp-not-inc", "Local-gov", "Notlisted", "State-gov", "Self-emp-inc", "Federal-gov"
# ]) # Based on your value counts after cleaning
# fnlwgt = st.sidebar.number_input("Fnlwgt", value=200000) # Example value, adjust as needed
# educational_num = st.sidebar.slider("Educational Number", 1, 16, 9) # Based on your data
# marital_status = st.sidebar.selectbox("Marital Status", [
#     "Married-civ-spouse", "Never-married", "Divorced", "Separated", "Widowed", "Married-spouse-absent", "Married-AF-spouse"
# ]) # Based on your value counts
# occupation = st.sidebar.selectbox("Occupation", [
#     "Prof-specialty", "Craft-repair", "Exec-managerial", "Adm-clerical", "Sales", "Other-service",
#     "Machine-op-inspct", "others", "Transport-moving", "Handlers-cleaners", "Farming-fishing",
#     "Tech-support", "Protective-serv", "Priv-house-serv", "Armed-Forces"
# ]) # Based on your value counts
# relationship = st.sidebar.selectbox("Relationship", [
#     "Husband", "Own-child", "Not-in-family", "Unmarried", "Wife", "Other-relative"
# ]) # Based on common values
# race = st.sidebar.selectbox("Race", [
#     "White", "Black", "Asian-Pac-Islander", "Amer-Indian-Eskimo", "Other"
# ]) # Based on common values
# gender = st.sidebar.selectbox("Gender", ["Male", "Female"])
# capital_gain = st.sidebar.number_input("Capital Gain", value=0)
# capital_loss = st.sidebar.number_input("Capital Loss", value=0)
# hours_per_week = st.sidebar.slider("Hours per week", 1, 99, 40) # Adjusted range based on typical values
# native_country = st.sidebar.selectbox("Native Country", [
#     "United-States", "Mexico", "Philippines", "Germany", "Puerto-Rico", "Canada",
#     "El-Salvador", "India", "Cuba", "England", "Jamaica", "South", "China",
#     "Italy", "Dominican-Republic", "Vietnam", "Guatemala", "Columbia", "Poland",
#     "Japan", "Greece", "Taiwan", "Haiti", "Iran", "Portugal", "Nicaragua",
#     "Peru", "Ecuador", "France", "Ireland", "Hong", "Thailand", "Cambodia",
#     "Trinadad&Tobago", "Laos", "Yugoslavia", "Outlying-US(Guam-USVI-etc)",
#     "Scotland", "Honduras", "Hungary"
# ]) # Based on common values
# 
# # Build input DataFrame
# input_df = pd.DataFrame({
#     'age': [age],
#     'workclass': [workclass],
#     'fnlwgt': [fnlwgt],
#     'educational-num': [educational_num],
#     'marital-status': [marital_status],
#     'occupation': [occupation],
#     'relationship': [relationship],
#     'race': [race],
#     'gender': [gender],
#     'capital-gain': [capital_gain],
#     'capital-loss': [capital_loss],
#     'hours-per-week': [hours_per_week],
#     'native-country': [native_country]
# })
# 
# st.write("### ðŸ”Ž Input Data")
# st.write(input_df)
# 
# # Apply the same preprocessing steps as in training
# # 1. Apply Label Encoding to categorical columns
# # for col, encoder in label_encoders.items():
# #     input_df[col] = encoder.transform(input_df[col])
# 
# # 2. Apply Scaling
# # input_df_scaled = scaler.transform(input_df)
# 
# 
# # Predict button
# if st.button("Predict Salary Class"):
#     # Use the scaled input_df for prediction
#     # prediction = model.predict(input_df_scaled)
#     # For now, predicting without scaling (will cause error if model expects scaled data)
#     prediction = model.predict(input_df)
#     st.success(f"âœ… Prediction: {prediction[0]}")
# 
# # Batch prediction
# st.markdown("---")
# st.markdown("#### ðŸ“‚ Batch Prediction")
# uploaded_file = st.file_uploader("Upload a CSV file for batch prediction", type="csv")
# 
# if uploaded_file is not None:
#     batch_data = pd.read_csv(uploaded_file)
#     st.write("Uploaded data preview:", batch_data.head())
# 
#     # Apply the same preprocessing steps to batch_data
#     # 1. Apply Label Encoding
#     # for col, encoder in label_encoders.items():
#     #     batch_data[col] = encoder.transform(batch_data[col])
# 
#     # 2. Apply Scaling
#     # batch_data_scaled = scaler.transform(batch_data)
# 
#     # Use the scaled batch_data for prediction
#     # batch_preds = model.predict(batch_data_scaled)
#     # For now, predicting without scaling (will cause error if model expects scaled data)
#     batch_preds = model.predict(batch_data)
# 
#     batch_data['PredictedClass'] = batch_preds
#     st.write("âœ… Predictions:")
#     st.write(batch_data.head())
#     csv = batch_data.to_csv(index=False).encode('utf-8')
#     st.download_button(
#         label="Download Predictions CSV",
#         data=csv,
#         file_name="predictions.csv",
#         mime="text/csv",
#     )

!pip install streamlit pyngrok

!ngrok authtoken 30YuqUBGAZjioVqTRCU41zXNvyh_6oFxjshwCBWVVMPfSZRxb

import os
import threading
def run_streamlit():
  os.system('streamlit run app.py --server.port8501')
  thread=threading.thread(target=run_streamlit)
  thread.dstart()

from pyngrok import ngrok
import time
time.sleep(5)
public_url= ngrok.connect(8501)
print("your stramlite is live here",public_url)